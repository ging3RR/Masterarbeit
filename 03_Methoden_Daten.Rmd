
<style> body {text-align: justify} </style> <!-- https://stackoverflow.com/questions/58713332/justify-text-in-rmarkdown-html-output-in-yaml-header -->

# Methoden

In diesem Kapitel folgt die Beschreibung des empirischen Materials sowie der verwendeten Methoden für die Analyse der Daten. In dieser Arbeit wird eine soziologische Sichtweise auf die Frage der Arbeitszeitverkürzung geworfen. Dabei steht die Diskussion um die Arbeitsverkürzung und speziell die erste und einzige gesamtschweizerische Rechtsgebung mit dem Arbeitsgesetz von 1964 im Fokus. Die Daten werden Mithilfe von Computergestützer Textanalyse bearbeitet. Somit befindet sich diese Arbeit zwischen Bereichen der Sozialwissenschaften, sowie Datenwissenschaften. Es wird also nebst der soziologischen Behandlung des Themas auch einen stärkeren Fokus auf die technischen Teil der Methoden eingegangen. Anschliessend an die Beschreibung des technischen Teils wird auf die (soziologische) Methoden und Vorgehensweise eingegangen.

## Technische Vorbermerkung

Ein wichtiger Aspekt für Forschung ist die Reproduzierbarkeit von Forschungsergebnissen. Erst durch die Reproduzierbarkeit können Ergebnisse nachgeprüft und von der Forschungsgesellschaft verifiziert werden. Einer der Grundpfeiler vieler Journals ist das sogenannte Peer-Reviewing. Es ist somit von grösster Wichtigkeit Forschungsarbeiten so zu verfassen, dass die Ergebnisse nicht nur nachvollziehbar, sondern eben auch reproduzierbar sind. Mit dem immer grösseren Zufluss von computergestützten Methoden in den Sozialwissenschaften erhält dieses Thema noch mehr Gewicht. Viele Algorithmen verwenden unsupervised oder semi-supervised Methoden und werden darum auch oft "Blackboxes" genannt. Doch auch wenn nicht immer ganz durchsichtig, können Vorkehrungen getroffen werden, um die Forschungsarbeiten und Ergebnisse zu reproduzieren. Für die Analyse in dieser Arbeit wurden einige solche Vorkehrungen getroffen. Ein wichtiger Punkt für die Reproduzierbarkeit ist die Transparenz der Methoden, aus diesem Grund wurde der ganze Code und Text in einem Github Repository veröffentlicht. Einzig die Daten dürfen nicht veröffentlicht werden, da dieser unter Copyright stehen. Im Teil weiter unten wird jedoch erklärt wo die Daten zur Verfügung stehen. Sobald die Daten zur Verfügung stehen, können jedoch alle Scripts und Methoden genau gleich angewandt werden. Des weiteren wird das Github Repository mit einer Docker Version zu Verfügung gestellt. Damit kann die virtuelle Umgebung in der gearbeitet wird nachgebaut werden. Dies verhindert unter anderem auch Fehler durch unterschiedliche Versionen von Softwares. 

Die ganze Arbeit wurde in RStudio geschrieben. Die Texte sind in RMarkdown geschrieben und werden mithilfe von `Bookdown` zu einem PDF Output zusammengefügt. Der Workflow für die Arbeit und das Github Repository stammt von Thea Knowles [@knowlesDissertatingRMarkdownBookdown2019] in Anlehnung an Lucy D'Agostino McGowan [@dagostinomcgowanOneYearDissertate2018]. Ein weiteres wichtiges Mittel für die Arbeit mit Bookdown in RStudio stammt aus der Dokumenation des Packages geschrieben von Yihui Xie, der das Package auch entwickelt hat [@xieBookdownAuthoringBooks2022]. Die Scripte für die Analyse sind zum grössten Teil in der R Programmiersprache geschrieben. Eines der Data Cleaning Scripts wurde in Python geschrieben, da eines der Probleme nicht in R gelöst werden konnte. Auch dieses Python Script ist innerhalb der RStudio Umgebung mithilfe des `reticulate` Packages ausführbar. Es ist möglich, die Analysen im vierten Teil dieser Arbeit nachrechnen zu lassen und allenfalls andere Parameter zu wählen, um bessere Forschungsergebnisse zu erhalten. Für diese Analyse wurde unter anderem das `LDA Prototype`Package von Rieger et al. verwendet[@riegerImprovingReliabilityLatent2020; @riegerLdaPrototypeMethodGet2020]. Dieses Package sorgt für bessere Reproduzierbarkeit bei LDA Analysen von Texten. 

Die *Latent dirichlet allocation* (LDA) ist eine Textanalyse Methode, die dazu dient die Dimensionen eines Korpus zu verringern [@10.5555/944919.944937]. In dieser Methoden werden alle Wörter in unterschiedliche Wortlisten zugeteilt. In diesen Wortliste, auch Topics genannt, werden die Wörter aufgrund eines Wahrscheinlichkeitswertes sortiert. Die Topwörter mit den höchsten Werten sollten idealerweise ein menschlich interpretierbares Thema innerhalb des Korpus repräsentieren. Die Anzahl der Topics kann vor der Analyse vorgegeben werden. Die Anzahl soll dabei so gewählt werden, dass nicht alle Themen der Texte in den Wortlisten vorkommen. Das Ziel soll eine Reduktion der Dimension bleiben. Dennoch müssen es genügend Topics sein, um als Grundlage für eine Analyse des Forschunggegenstands dienen zu können [@vonnordheimYoungFreeBiased2019a]. Die Anzahl der Topics wird also durch die Themen und die Komplexität des Korpus vorgegeben[@10.1145/2597008.2597150]. Da die Topics menschlich interpretierbar sein sollen, spielt auch die Vertrautheit mit dem empirischen Material eine wichtige Rolle, da sonst allfällige wichtige Wörter nicht erkannt werden. Weil in der Bestimmung der Wahrscheinlichkeiten der erste Schritt zufällig ist, kann das LDA Verfahren oft verschiedene Resultate ergeben. Um dies zu verhindern werden beim `LDA Prototype`Package mehrere Durchgängen gemacht. Von all diesen Durchgängen wird schlussendlich ein LDA Durchgang mit der grössten Ähnlichkeit zu allen anderen bestimmt. Diesen LDA Durchgang nennen Rieger et al. den Prototype. Diese Methode dient somit zur Absicherung und besseren Reproduzierbarkeit eines LDA Ergebnisses. Die Autor:innen empfehlen mindestens 50 Durchgänge ausrechnen zu lassen und dies je nach Komplexität der Korpora zu erhöhen. ^[Eine genaue technische Ausführung zu den LDA Methoden findet sich in den zitierten Texten von Blei et al. und Rieger et al.]


## Daten

Bevor genauer auf die Methoden eingegangen wird folgt hier noch eine Beschreibung der Daten, die für die Analysen genutzt worden sind. Wie im theoretischen Teil bereits angesprochen sind der Schweizerische Gewerkschaftsbund (SGB) und der Zentralverband schweizerischer Arbeitgeber-Organisationen (ZSAO) zwei der wichtigsten Verhandlungsteilnehmer rund um die Arbeitszeitdiskussion. Es war möglich für beide dieser Verbände den gesamten Bestand der Mitgliederzeitungen von 1930 bis 1970 zu erhalten. Der Erwerb der Zeitschriften war kostenpflichtig. Dies zeigt, dass die Zeitschriften vor allem an aktive und sehr interessierte Mitglieder gerichtet war. <br>
Die Mitgliederzeitung des SGB heisst Gewerkschaftliche Rundschau und ist in den Jahren 1909 bis 1946 monatlich und in den Jahren 1947 bis 1994 im Vierteljahrestakt herausgegeben worden. **MEHR INFOS ZUR ZEITUNG** Die digitalen Scans der Gewerkschaftlichen Rundschau sind auf dem E-Periodica Archiv der ETH Zürich öffentlich zugänglich. Für diese Arbeit hat der Autor die digitalisierten Texte im .txt Format erhalten. <br>
Die Zeitung des ZSAO ist jährlich erschienen und trägt den Namen Arbeitsgeber.**MEHR INFOS ZUR ZEITUNG** Die Zeitschriften sind vor Kurzem auch an das Archiv der ETHZ übergeben worden. Sie sind jedoch nicht öffentlich zugänglich. Eine digitalisierte Version der Zeitschriften konnte das Wirtschaftsarchiv der Universität Basel zu Verfügung stellen. Diese Texte waren im OCR Verfahren der Firma ... eingelesen worden. Als Output dieses Verfahrens entstehen XML Dateien. Wie bereits angesprochen mussten für das Einlesen dieser Texte in RStudio ein Python Script verwendet werden. Der Grund dafür war die Struktur des XML, in dem der Textinhalt in einer verschachtelten Child Node zu finden war. In der R Sprache konnte dieses Problem leider nicht gelöst werden, da der gewünschte Synthax nicht herausgefunden werden konnte. Da dieses Problem einfacher in Python gelöst werden konnte, wurde schlussendlich zu dieser weniger eleganten Lösung zurückgegriffen. 

**ZUSAMMENFASSENDE INFORMATIONEN ZU DEN TEXTEN** 

Da das Thema dieser Arbeit der Diskurs um die Arbeitszeitverkürzung ist, war es sinnvoll das empirische Material vorzusortieren. In den Zeitungen werden viele verschiedene Themen angeschnitten und diskutiert. Daher wurden die Texte so vorsotiert, dass nur Texte mit dem Wort Arbeitszeit in die zwei Analyse-Korpora integriert worden sind. Für diese Auswahl wurde eine *Keywords in context* (KWIC) Analyse mit dem Schlagwort "Arbeitszeit" durchgeführt. Die KWIC Analyse wurde mithilfe des `quanteda` Packages durchgeführt. Bei dieser Analyse werden die Texte nach dem gewünschten Wort durchsucht. Als Ausgabe wird das Wort und einige Wörter vor und nach dem Wort ausgegeben. In diesem Moment war der Kontext des Wortes Arbeitszeit aber noch eher unwichtig. Zusätzlich wird in der Ausgabe noch die ID des Textes angegeben. Diese ID's wurden zu einer Liste zusammengefasst und mit dieser Liste wurden dann alle Texte aussortiert. Schlussendlich blieben bei beiden Korpora nur noch Texte übrig in denen mindestens einmal das Wort Arbeitszeit vorkommt. 

**HIER INFOS ZU DIMENSIONEN DER AUSSORTIERTEN KORPORA** 