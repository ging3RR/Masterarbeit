# Methoden

Nach der theoretischen Behandlung des Themas im vorherigen Kapitel wird in diesem Kapitel das empirische Material sowie die verwendeten Methoden für die Analyse der Daten beschrieben. Im Fokus steht die Analyse der Diskussion um die Arbeitsverkürzung im Zeitraum der ersten und einzigen gesamtschweizerische Rechtsgebung: dem Arbeitsgesetz von 1964. Als Grundlage für die Auswertungen werden die Mitgliederzeitschriften der beiden Verbände SGB und ZSAO verwendet. Die zwei Verbände waren als Ordnungskräfte massgeblich an der politischen Debatte sowie an der rechtlichen Umsetzung von Arbeitszeitregulierungen beteiligt. Die Zeitschriften werden mithilfe von computergestützter Textanalyse bearbeitet.

Ein wichtiger Aspekt für wissenschaftliche Forschung ist die Reproduzierbarkeit von Forschungsergebnissen. Erst durch die Reproduzierbarkeit können Ergebnisse nachgeprüft und von der Forschungsgesellschaft verifiziert werden. Nicht ohne Grund ist einer der Grundpfeiler vieler Journals das sogenannte Peer-Reviewing. Es ist somit von grösster Wichtigkeit, Forschungsarbeiten so zu verfassen, dass die Ergebnisse nicht nur nachvollziehbar, sondern eben auch reproduzierbar sind [@granellReproducibleResearchRiding2018; @peelsPossibilityDesirabilityReplication2018]. Mit der zunehmenden Ausbreitung von computergestützten Methoden in den Sozialwissenschaften erhält dieses Thema zusätzlich Gewicht. Viele Algorithmen verwenden unsupervised oder semi-supervised Methoden und werden darum auch oft „Blackboxes“ genannt [für eine Einführung wie solche Algorithmen funktionieren siehe @oneilDoingDataScience2013]. Doch auch wenn nicht immer ganz durchsichtig, können Vorkehrungen getroffen werden, um die Forschungsarbeiten und Ergebnisse zu reproduzieren. Für die Analyse in dieser Arbeit wurden einige solche Vorkehrungen getroffen. Ein wichtiger Punkt für die Reproduzierbarkeit ist die Transparenz der Methoden. Aus diesem Grund wurde der ganze Code und Text in einem Github-Repository veröffentlicht [@bakerWhyScientistMust2016]. Einzig die Daten dürfen nicht veröffentlicht werden, da diese unter Copyright stehen. Im Kapitel 3.1 wird jedoch darauf eingegangen, wo die Daten zur Verfügung stehen. 

Die ganze Arbeit wurde in RStudio verfasst und programmiert. Die Texte sind in RMarkdown geschrieben und werden mithilfe von `Bookdown` zu einem PDF-Output zusammengefügt. Der Workflow für die Arbeit und das Github-Repository stammt von Thea Knowles [@knowlesDissertatingRMarkdownBookdown2019] in Anlehnung an Lucy D'Agostino McGowan [@dagostinomcgowanOneYearDissertate2018]. Ein weiteres wichtiges Mittel für die Arbeit mit `Bookdown` in RStudio stammt aus dessen Dokumenation geschrieben von Yihui Xie, der das Package auch entwickelt hat [@xieBookdownAuthoringBooks2022]. Die Scripts für die Analyse sind zum grössten Teil in der R-Programmiersprache geschrieben [@rcoreteamLanguageEnvironmentStatistical2022]. Eines der Data-Cleaning-Scripts wurde in Python geschrieben, da ein Problem nicht in R gelöst werden konnte. Auch dieses Python-Script ist innerhalb der RStudio Umgebung mithilfe des `reticulate-Packages` ausführbar [@R-reticulate]. Ein grosser Vorteil dieses Workflows ist die einfache Anpassung der Analysen. Es ist möglich, die Analysen des vierten Teils dieser Arbeit nachrechnen zu lassen und allenfalls andere Parameter zu wählen, um bessere Forschungsergebnisse zu erhalten. Damit können immer wieder neue Erkenntnisse in die Analysen miteinbezogen werden, ohne noch einmal alles von Grund auf programmieren zu müssen. Ausserdem können dieselben Verfahren für anderes empirisches Material genützt werden. Dies ermöglicht es wiederum, verschiedene Materialien zu vergleichen und die Methoden auf ihre Generalisierbarkeit zu überprüfen.

## Daten

Bevor genauer auf die Methoden eingegangen wird, folgt hier eine Beschreibung der Daten, die für die Analysen genutzt worden sind. Wie im theoretischen Teil bereits angesprochen sind der Schweizerische Gewerkschaftsbund (SGB) und der Zentralverband Schweizerischer Arbeitgeber-Organisationen (ZSAO) zwei der wichtigsten Verhandlungsteilnehmer:innen rund um die Arbeitszeitdiskussion. Der Erwerb der Zeitschriften war zu seiner Zeit kostenpflichtig. Dies zeigt, dass die Zeitschriften vor allem an aktive und interessierte Mitglieder gerichtet waren. Es unterscheidet sich somit von einem Flugblatt oder politischen Abstimmungswerkzeugen wie z.B. Plakaten. Für diese Arbeit wurde der gesamte Bestand der Mitgliederzeitungen beider Verbände von 1930 bis 1970 analysiert.
Die Mitgliederzeitung des SGB heisst *Gewerkschaftliche Rundschau* und wurde im Untersuchungszeitraum monatlich herausgegeben. Erst ab den 1980er-Jahren wurde die Rundschau dann zweimonatlich und schlussendlich vierteljährlich veröffentlicht. Die digitalen Scans der *Gewerkschaftlichen Rundschau* sind auf dem E-Periodica-Archiv der ETH Zürich öffentlich zugänglich. Die einzelnen Ausgaben werden in Bänden pro Jahr zusammengefasst. Für diese Arbeit hat der Autor die digitalisierten Texte im txt-Format direkt vom Archiv erhalten. <br>
Die Zeitung des ZSAO ist jährlich erschienen und trägt den Namen *Schweizer Arbeitgeber*. Die Zeitschriften sind vor Kurzem an das Archiv für Zeitgeschichte an der ETH Zürich übergeben worden. Sie sind jedoch nicht öffentlich zugänglich. Eine digitalisierte Version der Zeitschriften konnte das Wirtschaftsarchiv der Universität Basel zur Verfügung stellen. Diese Texte wurden mit einem OCR-Verfahren mithilfe der Tesseract-Software verarbeitet. Als Output dieses Verfahrens entstehen XML-Dateien. 

Die beiden Korpora sind von unterschiedlicher Grösse. Die *Gewerkschaftliche Rundschau* wurde in den Jahren 1930 – 1970 rund 480 Mal herausgegeben. Die Zeitschrift *Schweizer Arbeitgeber* wurde jeweils einmal pro Jahr veröffentlicht. Somit gibt es 40 Ausgaben dieser Zeitschrift. Bei beiden Korpora entspricht eine Datei (txt oder XML) jeweils einer Seite eines Hefts. Für die *Gewerkschaftliche Rundschau* sind für die Jahre 1930 – 1970 insgesamt rund 18'000 Dateien vorhanden. Dies entspricht 450 Seiten pro Jahr oder 37.5 Seiten pro Monat. Beim *Schweizer Arbeitgeber* sind es insgesamt rund 59’000 Dateien – also 1’475 Seiten pro Jahr und ca. 123 Seiten pro Monat. Der Korpus des ZSAO ist entsprechend rund dreimal grösser als der des SGB.

Da das Thema dieser Arbeit der Diskurs um die Arbeitszeitverkürzung ist, war es sinnvoll das empirische Material vorzusortieren. In den Zeitungen wurden viele verschiedene Themen angeschnitten und diskutiert. Daher wurden die Texte so vorsortiert, dass nur Texte mit dem Wort „Arbeitszeit“ in die zwei Analyse-Korpora integriert worden sind. Für diese Auswahl wurde eine „Keywords in context“ (KWIC) Analyse mit dem Schlagwort „Arbeitszeit“ durchgeführt. Die KWIC-Analyse wurde mithilfe des `quanteda-Packages` durchgeführt [@benoitQuantedaPackageQuantitative2018]. Bei dieser Analyse werden die Texte nach dem gewünschten Wort durchsucht. Ausgegeben werden das Wort und fünf Wörter vor und nach dem Wort. Zusätzlich wird in der Ausgabe die ID des Textes angegeben. Diese IDs wurden zu einer Liste zusammengefasst und anhand derer schliesslich alle Texte aussortiert. So blieben bei beiden Korpora nur noch Texte übrig, in denen mindestens einmal das Wort Arbeitszeit vorkommt.^[Auf dem Github-Repository dieser Arbeit sind die Scripts für die Auswahl unter */scripts* zu finden. Für die *Gewerkschaftliche Rundschau* wurde das Script *Aussortieren* benutzt. Beim *Schweizer Arbeitgeber* ist das Aussortieren im ersten Teil des *Arbeitgeber_2*-Scripts.]

Nach dem Aussortieren blieben beim Korpus des SGB noch 1'027 (5.7%) Dateien übrig. Beim Korpus des ZSAO noch 2'217 (3.7%) Dateien. In beiden Korpora ist aus jedem Jahr mindestens ein Text vorhanden. Das Wort „Arbeitszeit“ wurde also mindestens einmal pro Jahr in den Zeitschriften erwähnt. Die Grösse der Dateien ist in den beiden Korpora unterschiedlich. Durchschnittlich hat eine Datei der *Gewerkschaftlichen Rundschau* 365 Tokens. Der Durchschnitt beim *Schweizer Arbeitgeber* ist rund 1'100 Tokens. Der Korpus des ZSAO bleibt also um einiges grösser als der des SGB.
Bei der Umwandlung in ein Korpus-Objekt werden die verschiedenen Zeichen und Wörter aus den Texten entfernt. Was genau entfernt wird, kann jeweils mit Parametern angegeben werden. Bei beiden Korpora wurden Sonderzeichen und eine Liste von deutschen und französischen Stopwörtern entfernt. Dafür wird eine Liste von Stopwörtern (oft verwendete Wörter ohne interpretierbaren Inhalt wie z.B. Konjunktionen oder Artikel) aus dem `Stopwords-Package` aus den Texten entfernt [@muhrStopwordsPackage2021].^[Stopwörter müssen nicht immer entfernt werden. Je nach Analyse ist es sogar nötig, die Stopwörter nicht zu entfernen, zum Beispiel bei der Identifizierung von Autor:innen, in denen die Verhältnisse von bestimmten Wortgruppen miteinander verglichen werden [@raghunadhareddyNewDocumentRepresentation2019; @reddy2016survey]. Für die explorative Untersuchung von Dimension in Texten sind Stopwörter aber störend, da sie keine Interpretation zu den Themen in den Texten zulassen.] Weitere störende Wörter, die durch die Analysen identifiziert werden konnten, wurden nach und nach hinzugefügt. Eine genaue Übersicht über die Stopwörter findet sich im Script *stopwords.R*. 


## Analyseverfahren

Die *Latent Dirichlet allocation* (LDA) ist eine Textanalyse-Methode, die dazu dient, die Dimensionen eines Korpus zu verringern [@10.5555/944919.944937]. In dieser Methode werden alle Wörter in unterschiedliche Wortlisten eingeteilt. In diesen Wortlisten, auch „Topics“ genannt, werden die Wörter aufgrund eines Wahrscheinlichkeitswertes sortiert. Die Topwörter mit den höchsten Werten sollten idealerweise ein menschlich interpretierbares Thema innerhalb des Korpus repräsentieren. Die Anzahl der Topics kann vor der Analyse vorgegeben werden. Die Anzahl soll dabei so gewählt werden, dass nicht alle Themen der Texte in den Wortlisten vorkommen, da das Ziel eine Reduktion der Dimension bleibt. Dennoch müssen es genügend Topics sein, um als Grundlage für eine Analyse des Forschungsgegenstands dienen zu können [@vonnordheimYoungFreeBiased2019a]. Die Anzahl der Topics wird durch die Themen und die Komplexität des Korpus vorgegeben [@10.1145/2597008.2597150]. Da die Topics menschlich interpretierbar sein sollen, spielt auch die Vertrautheit mit dem empirischen Material eine wichtige Rolle, da sonst allfällige wichtige Wörter nicht erkannt werden. Weil in der Bestimmung der Wahrscheinlichkeiten der erste Schritt zufällig ist, kann das LDA-Verfahren oft verschiedene Resultate ergeben.^[Eine genaue technische Ausführung zu den LDA-Methoden findet sich in den zitierten Texten von Blei et al. und Rieger et al.] Um dies zu verhindern werden beim `LDA-Prototype-Package` mehrere Durchgänge gemacht. Von all diesen Durchgängen wird schliesslich der LDA-Durchgang mit der grössten Ähnlichkeit zu allen anderen bestimmt. Diesen LDA-Durchgang nennen Rieger et al. den Prototype. Diese Methode dient zur Absicherung und besseren Reproduzierbarkeit eines LDA-Ergebnisses [@riegerImprovingReliabilityLatent2020; @riegerLdaPrototypeMethodGet2020]. Die Autor:innen empfehlen mindestens 50 Durchgänge ausrechnen zu lassen und dies je nach Komplexität der Korpora zu erhöhen.

Da es keine objektiven Kriterien zur Festlegung der Anzahl Topics gibt, ist die Überprüfung der Validität der Ergebnisse absolut zentral. Eine Möglichkeit ist, die Topics auf ihre intersubjektive Nachvollziehbarkeit und ihre Konsistenz zu überprüfen. Das `Tosca-Package` [@jonas_rieger_2020_3703066] erlaubt eine solche Validitätsprüfung mit der Funktion `intruderwords`. Dabei wird ein Konzept von Chang et al. verwendet, dass sich  „reading tealeaves“ nennt [@10.5555/2984093.2984126]. Bei diesem Verfahren wird eine Auswahl aus Wörtern aus einem bestimmten Topic gezeigt. Unter dieser Auswahl verstecken sich auch (ein oder mehrere) Wörter, die nicht in diesem Topic zu finden sind. Diese Wörter sind die sogenannten Intruderwords. Bei einem gut abgrenzbaren Topic sollte es möglich sein, die Intruderwords zu erkennen. Ist das Topic schlecht abgrenzbar, kann nicht unterschieden werden, welche Wörter wirklich zum Topic gehören und welche nicht. Bei der Funktion aus dem `Tosca-Package` können einige Parameter angegeben werden. Es werden pro Durchgang jeweils fünf Wörter angezeigt. Die Anzahl der Intruder wurde auf ein Wort angesetzt. Weiter kann festgelegt werden, wie viele der Topwords genützt werden sollen. Hier wurde der Parameter auf zehn Topwords eingestellt. Die Interpretation der Topics erfolgte jeweils mithilfe dieser zehn Topwords. Daher macht es auch Sinn, die Validität mit den zehn Topwords zu überprüfen. Ein Durchgang umfasst jeweils so viele Stichproben, wie es Topics gibt. Es gibt jedoch die Möglichkeit, ein nicht interpretierbares Topic zu kennzeichnen. Wird dies als solches gekennzeichnet, gibt es eine zusätzliche Stichprobe. Ein Durchgang kann also je nachdem auch mehr Stichproben als die Anzahl Topics haben.
Durchgeführt wurde die Validitätsprüfung von drei verschiedenen Personen. Eine Person war der Autor dieser Arbeit. Die anderen zwei Personen haben sich nicht mit der Arbeitszeitverkürzung auseinandergesetzt. Dadurch soll überprüft werden, ob das Kontextwissen bei der Interpretation der Topics eine Hilfe ist. Für die Validitätsprüfung wurden jeweils drei Durchgänge pro Person durchgeführt. 

Eine weitere verwendete Methode ist die Sentiment-Analyse mithilfe eines Wörterbuchs. Wie politische Organisationen über bestimmte Themen reden ist eine wichtige Informationsquelle zur Interpretation des Themas [@rauhValidatingSentimentDictionary2018, S.319]. Die Sentiment-Analyse wird in unterschiedlichen Forschungsdisziplinen angewendet [z.B. Sportwissenschaften @app10020431; oder Gesundheitswissenschaften @yangModellingClinicalExperience2020]. Bei der Sentiment Analyse werden alle Wörter in eine positive oder negative Kategorie eingeteilt.^[Einige Wörterbücher verwenden auch eine dritte Kategorie für neutrale Wörter. Siehe dafür zum Beispiel das *Lexicoder Sentiment Dictionary* [@youngAffectiveNewsAutomated2012].] Dies geschieht mithilfe des Wörterbuchs, in dem die Wörter zusammen mit einem positiven oder negativen Wert eingetragen sind. Die Wörter aus den Texten werden mit dem Wörterbuch gematcht und der Wert wird übertragen. Anschliessend lässt sich durch das Aufsummieren und Zusammenfassen sagen, ob der Text/Korpus insgesamt eher negativ oder positiv ist. In dieser Arbeit wird dafür ein deutsches Wörterbuch von Christian Rauh [@rauhValidatingSentimentDictionary2018] verwendet, das auf Grundlage des *SentiWS*-Wörterbuchs der Universität Leipzig gebaut wurde [@goldhahn-etal-2012-building]. Für die deutsche Sprache gibt es kaum andere Wörterbücher als das SentiWS und das Wörterbuch von Rauh. Das Wörterbuch von Rauh fokussiert sich auf politische Kommunikation und wurde für diesen Bereich validiert. Da es sich bei den Zeitschriften der beiden Verbände um politische Kommunikation handelt, können sie mit dem Wörterbuch von Rauh analysiert werden. Das Ziel der Sentiment-Analyse ist herauszufinden, ob das Thema Arbeitszeit in der Kommunikation der Gewerkschaften und den Arbeitgeber:innenorganisationen positiv oder negativ bewertet wurde.
