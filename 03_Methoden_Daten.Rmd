

# Methoden

In diesem Kapitel folgt die Beschreibung des empirischen Materials sowie der verwendeten Methoden für die Analyse der Daten. In dieser Arbeit wird eine soziologische Sichtweise auf die Frage der Arbeitszeitverkürzung geworfen. Dabei steht die Diskussion um die Arbeitsverkürzung und speziell die erste und einzige gesamtschweizerische Rechtsgebung mit dem Arbeitsgesetz von 1964 im Fokus. Die Daten werden Mithilfe von Computergestützer Textanalyse bearbeitet. Somit befindet sich diese Arbeit zwischen Bereichen der Sozialwissenschaften, sowie Datenwissenschaften. Es wird also nebst der soziologischen Behandlung des Themas auch einen stärkeren Fokus auf die technischen Teil der Methoden eingegangen. Anschliessend an die Beschreibung des technischen Teils wird auf die (soziologische) Methoden und Vorgehensweise eingegangen.

## Technische Vorbermerkung

Ein wichtiger Aspekt für Forschung ist die Reproduzierbarkeit von Forschungsergebnissen. Erst durch die Reproduzierbarkeit können Ergebnisse nachgeprüft und von der Forschungsgesellschaft verifiziert werden. Einer der Grundpfeiler vieler Journals ist das sogenannte Peer-Reviewing. Es ist somit von grösster Wichtigkeit Forschungsarbeiten so zu verfassen, dass die Ergebnisse nicht nur nachvollziehbar, sondern eben auch reproduzierbar sind. Mit dem immer grösseren Zufluss von computergestützten Methoden in den Sozialwissenschaften erhält dieses Thema noch mehr Gewicht. Viele Algorithmen verwenden unsupervised oder semi-supervised Methoden und werden darum auch oft "Blackboxes" genannt. Doch auch wenn nicht immer ganz durchsichtig, können Vorkehrungen getroffen werden, um die Forschungsarbeiten und Ergebnisse zu reproduzieren. Für die Analyse in dieser Arbeit wurden einige solche Vorkehrungen getroffen. Ein wichtiger Punkt für die Reproduzierbarkeit ist die Transparenz der Methoden, aus diesem Grund wurde der ganze Code und Text in einem Github Repository veröffentlicht. Einzig die Daten dürfen nicht veröffentlicht werden, da dieser unter Copyright stehen. Im Teil weiter unten wird jedoch erklärt wo die Daten zur Verfügung stehen. Sobald die Daten zur Verfügung stehen, können jedoch alle Scripts und Methoden genau gleich angewandt werden. Des weiteren wird das Github Repository mit einer Docker Version zu Verfügung gestellt. Damit kann die virtuelle Umgebung in der gearbeitet wird nachgebaut werden. Dies verhindert unter anderem auch Fehler durch unterschiedliche Versionen von Softwares. 

Die ganze Arbeit wurde in RStudio geschrieben. Die Texte sind in RMarkdown geschrieben und werden mithilfe von `Bookdown` zu einem PDF Output zusammengefügt. Der Workflow für die Arbeit und das Github Repository stammt von Thea Knowles [@knowlesDissertatingRMarkdownBookdown2019] in Anlehnung an Lucy D'Agostino McGowan [@dagostinomcgowanOneYearDissertate2018]. Ein weiteres wichtiges Mittel für die Arbeit mit Bookdown in RStudio stammt aus der Dokumenation des Packages geschrieben von Yihui Xie, der das Package auch entwickelt hat [@xieBookdownAuthoringBooks2022]. Die Scripte für die Analyse sind zum grössten Teil in der R Programmiersprache geschrieben. Eines der Data Cleaning Scripts wurde in Python geschrieben, da eines der Probleme nicht in R gelöst werden konnte. Auch dieses Python Script ist innerhalb der RStudio Umgebung mithilfe des `reticulate` Packages ausführbar. Es ist möglich, die Analysen im vierten Teil dieser Arbeit nachrechnen zu lassen und allenfalls andere Parameter zu wählen, um bessere Forschungsergebnisse zu erhalten. Für diese Analyse wurde unter anderem das `LDA Prototype` Package von Rieger et al. verwendet[@riegerImprovingReliabilityLatent2020; @riegerLdaPrototypeMethodGet2020]. Dieses Package sorgt für bessere Reproduzierbarkeit bei LDA Analysen von Texten. 

Die *Latent dirichlet allocation* (LDA) ist eine Textanalyse Methode, die dazu dient die Dimensionen eines Korpus zu verringern [@10.5555/944919.944937]. In dieser Methoden werden alle Wörter in unterschiedliche Wortlisten zugeteilt. In diesen Wortliste, auch Topics genannt, werden die Wörter aufgrund eines Wahrscheinlichkeitswertes sortiert. Die Topwörter mit den höchsten Werten sollten idealerweise ein menschlich interpretierbares Thema innerhalb des Korpus repräsentieren. Die Anzahl der Topics kann vor der Analyse vorgegeben werden. Die Anzahl soll dabei so gewählt werden, dass nicht alle Themen der Texte in den Wortlisten vorkommen. Das Ziel soll eine Reduktion der Dimension bleiben. Dennoch müssen es genügend Topics sein, um als Grundlage für eine Analyse des Forschunggegenstands dienen zu können [@vonnordheimYoungFreeBiased2019a]. Die Anzahl der Topics wird also durch die Themen und die Komplexität des Korpus vorgegeben[@10.1145/2597008.2597150]. Da die Topics menschlich interpretierbar sein sollen, spielt auch die Vertrautheit mit dem empirischen Material eine wichtige Rolle, da sonst allfällige wichtige Wörter nicht erkannt werden. Weil in der Bestimmung der Wahrscheinlichkeiten der erste Schritt zufällig ist, kann das LDA Verfahren oft verschiedene Resultate ergeben. Um dies zu verhindern werden beim `LDA Prototype` Package mehrere Durchgängen gemacht. Von all diesen Durchgängen wird schlussendlich ein LDA Durchgang mit der grössten Ähnlichkeit zu allen anderen bestimmt. Diesen LDA Durchgang nennen Rieger et al. den Prototype. Diese Methode dient somit zur Absicherung und besseren Reproduzierbarkeit eines LDA Ergebnisses. Die Autor:innen empfehlen mindestens 50 Durchgänge ausrechnen zu lassen und dies je nach Komplexität der Korpora zu erhöhen. ^[Eine genaue technische Ausführung zu den LDA Methoden findet sich in den zitierten Texten von Blei et al. und Rieger et al.]

Eine weitere verwendete Analyse Methode ist die Sentiment Analyse mithilfe eines Wörterbuchs. Bei der Sentiment Analyse werden alle Wörter in eine positive oder negative Kategorie eingeteilt. ^[Einige Wörterbücher verwenden auch eine dritte Kategorie für neutrale Wörter. Siehe dafür zum Beispiel das Lexicoder Sentiment Dictionary[@youngAffectiveNewsAutomated2012]] Dies geschieht mithilfe des Wörterbuchs, in dem viele Wörter zusammen mit einem positiven oder negative Wert eingetragen sind. Die Wörter aus den Texten werden mit dem Wörterbuch gematcht und der Wert wird übertragen. Anschliessend lässt sich durch das Aufsummieren und zusammenfassen sagen, ob der Text/Korpus insgesamt eher negativ oder positiv ist. Dafür wird ein deutsches Wörterbuch von Christian Rauh [@rauhValidatingSentimentDictionary2018] verwendet, das auf Grundlage des SentiWS Wörterbuchs der Universität Leipzig gebaut wurde [@goldhahn-etal-2012-building]. Für die deutsche Sprache gibt es fast keine anderen Wörterbücher als das SentiWS und das Wörterbuch von Rauh. Daher ist die Validität dieser Wörterbücher extrem wichtig. Für eine genaue Diskussion zur Validität kann der Text von Rauh beigezogen werden. 
Wie politischen Organisationen über bestimmte Themen reden, sind eine wichtige Informationsquelle zur Interpretation des Themas [@rauhValidatingSentimentDictionary2018, S.319]. Die Sentiment Analyse wird in unterschiedlichen Forschungsdisziplinen angewendet [z.B. Sportwissenschaften @app10020431; oder Gesundheitswissenschaften @yangModellingClinicalExperience2020]. Ein wichtiger Punkt bei den Sentiment Analyse mit einem Wörterbuch ist die Validität des Wörterbuches. Das Wörterbuch von Rauh fokussiert sich glücklicherweise auf politische Kommunikation. Die Kommunikation in den Zeitschriften der Gewerkschaften und Arbeitgeber:innenverbänden kann als politische Kommunikation kategorisiert werden. Die Validität für dieses Wörterbuch ist im Forschungskontext dieser Arbeit somit besser gegeben, als wenn es sich um nicht-politische Kommunikation handeln würde. Durch die Sentiment Analyse kann herausgefunden werden, ob die Gewerkschaften und die Arbeitgeber-Organisationen positiv oder negativ über das Thema Arbeitszeit kommunizieren.


## Daten

Bevor genauer auf die Methoden eingegangen wird folgt hier noch eine Beschreibung der Daten, die für die Analysen genutzt worden sind. Wie im theoretischen Teil bereits angesprochen sind der Schweizerische Gewerkschaftsbund (SGB) und der Zentralverband schweizerischer Arbeitgeber-Organisationen (ZSAO) zwei der wichtigsten Verhandlungsteilnehmer rund um die Arbeitszeitdiskussion. Es war möglich für beide dieser Verbände den gesamten Bestand der Mitgliederzeitungen von 1930 bis 1970 zu erhalten. Der Erwerb der Zeitschriften war kostenpflichtig. Dies zeigt, dass die Zeitschriften vor allem an aktive und sehr interessierte Mitglieder gerichtet war. <br>
Die Mitgliederzeitung des SGB heisst *Gewerkschaftliche Rundschau* und wurde im Untersuchungszeitraum monatlich herausgegeben. Erst ab den 80er Jahren wurde die Rundschau dann zweimonatlich und schlussendlich vierteljährlich veröffentlicht. **MEHR INFOS ZUR ZEITUNG** Die digitalen Scans der Gewerkschaftlichen Rundschau sind auf dem E-Periodica Archiv der ETH Zürich öffentlich zugänglich. Die einzelnen Ausgaben, auch Hefte genannt, werden in Bändern pro Jahr zusammengefasst. Für diese Arbeit hat der Autor die digitalisierten Texte im .txt Format erhalten. <br>
Die Zeitung des ZSAO ist jährlich erschienen und trägt den Namen *Schweizer Arbeitgeber*.**MEHR INFOS ZUR ZEITUNG** Die Zeitschriften sind vor Kurzem auch an das Archiv der ETHZ übergeben worden. Sie sind jedoch nicht öffentlich zugänglich. Eine digitalisierte Version der Zeitschriften konnte das Wirtschaftsarchiv der Universität Basel zu Verfügung stellen. Diese Texte waren im OCR Verfahren der Firma ... eingelesen worden. Als Output dieses Verfahrens entstehen XML Dateien. Wie bereits angesprochen mussten für das Einlesen dieser Texte in RStudio ein Python Script verwendet werden. Der Grund dafür war die Struktur des XML, in dem der Textinhalt in einer verschachtelten Child Node zu finden war. In der R Sprache konnte dieses Problem leider nicht gelöst werden, da der gewünschte Synthax nicht herausgefunden werden konnte. Da dieses Problem einfacher in Python gelöst werden konnte, wurde schlussendlich zu dieser weniger eleganten Lösung zurückgegriffen. 

**ZUSAMMENFASSENDE INFORMATIONEN ZU DEN TEXTEN** 
Die beiden Korpus sind von unterschiedlicher Grösse. Die *Gewerkschaftliche Rundschau* wurde in diesen Jahren rund 480 Mal herausgegeben. Eine Ausgabe hatte dabei zwischen 30 - 50 Seiten. Die Zeitschrift *Schweizer Arbeitgeber* wurde jeweils einmal pro Jahr veröffentlicht. Eine Ausgabe war jeweils rund 1'000 Seiten lang. Dies bedeutet im Schnitt 80 Seiten pro Monat. Der Korpus des Arbeitgebers ist deshalb auch um einiges grösser, als derjenige der Gewerkschaftlichen Rundschau. Bei beiden Korpora entspricht eine Datei jeweils einer Seite des Hefts. Für die *Gewerkschaftliche Rundschau* sind für die Jahre 1930 - 1970 insgesamt ... Dateien vorhanden. Beim *Schweizer Arbeitgeber* sind es insgesamt ...  

Da das Thema dieser Arbeit der Diskurs um die Arbeitszeitverkürzung ist, war es sinnvoll das empirische Material vorzusortieren. In den Zeitungen werden viele verschiedene Themen angeschnitten und diskutiert. Daher wurden die Texte so vorsortiert, dass nur Texte mit dem Wort Arbeitszeit in die zwei Analyse-Korpora integriert worden sind. Für diese Auswahl wurde eine *Keywords in context* (KWIC) Analyse mit dem Schlagwort "Arbeitszeit" durchgeführt. Die KWIC Analyse wurde mithilfe des `quanteda` Packages durchgeführt. Bei dieser Analyse werden die Texte nach dem gewünschten Wort durchsucht. Als Ausgabe wird das Wort und einige Wörter vor und nach dem Wort ausgegeben. In diesem Moment war der Kontext des Wortes Arbeitszeit aber noch eher unwichtig. Zusätzlich wird in der Ausgabe noch die ID des Textes angegeben. Diese ID's wurden zu einer Liste zusammengefasst und mit dieser Liste wurden dann alle Texte aussortiert. Schlussendlich blieben bei beiden Korpora nur noch Texte übrig in denen mindestens einmal das Wort Arbeitszeit vorkommt. ^[Auf dem Github Repository dieser Arbeit sind die Scripte für die Auswahl unter /scripts zu finden. Für die *Gewerkschaftliche Rundschau* wurde das Script *Aussortieren* benutzt. Beim *Schweizer Arbeitgeber* ist das Aussortieren im ersten Teil des *Arbeitgeber* Script.]

**HIER INFOS ZU DIMENSIONEN DER AUSSORTIERTEN KORPORA**
Nach dem Aussortieren blieben beim Korpus des SGB noch ... Dateien übrig. Beim Korpus des ZSAO noch ...